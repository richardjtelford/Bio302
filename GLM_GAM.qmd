---
title: Generalised Linear Models<br>&<br> Generalised Additive Models
subtitle: "Bio302 - Biological data analysis II"
author: 
- name: "Richard J. Telford"
institute: Department of Biology, University of Bergen
date: today
date-format: "MMMM YYYY"
format: revealjs
editor: visual
echo: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(mgcv)
theme_set(theme_bw(base_size = 18))
```

## Assumptions of Least Squares

1. The relationship between the response and the predictors is ~linear.
2. The residuals have a mean of zero.
3. __The residuals have constant variance (not heteroscedastic).__
4. The residuals are independent (uncorrelated).
5. __The residuals are normally distributed.__

Count data, binomial and proportion data do not have normally distributed errors

## Least squares

```{r, echo = FALSE, fig.height=4}
dat <- iris %>% filter(Species == "setosa")
mod <- lm(Sepal.Length ~ Sepal.Width, data = dat)
mod2 <- augment(mod)
ggplot(mod, aes(x = Sepal.Width, y = Sepal.Length)) +
  geom_point() +
  geom_line(data = mod2, aes(y = .fitted), colour = "red") 
```

$y_i = \beta_0 + \beta_1x_i + \epsilon_i$

Find $\beta_0$ and $\beta_1$ that minimise $\sum\epsilon ^ 2$

## Maximum Likelihood

How likely are the data given the model? – choose coefficients to maximise likelihood 

Residuals come from a distribution

Normal distribution: $L(y;\mu,\sigma) = \frac{1}{\sigma \sqrt {2\pi } }e^{-\frac{(x - \mu)^2} {{2\sigma ^2 }}}$

```{r, fig.height = 3.3, echo=FALSE}
Sepal.Width = 4
mu <- predict(mod, newdata = data.frame(Sepal.Width = Sepal.Width))
sigma <- sqrt(sum(resid(mod) ^ 2) / mod$df.residual)

tibble(x = seq(4.5, 6.5, .01), y = dnorm(x, mean = mu, sd = sigma)) %>% 
  ggplot(aes(x, y)) + 
  geom_line() + 
  geom_vline(xintercept = mu, colour = "grey40") +
  geom_vline(data = dat %>% filter(Sepal.Width == !!Sepal.Width) %>% slice(1), aes(xintercept = Sepal.Length), linetype = "dashed", colour = "grey40") +
  labs(subtitle =paste("Sepal.Width =", Sepal.Width), y = "Likelihood", x = "Sepal.Length")
```

## Maximum Likelihood

For each observation find the likelihood

Find the product of the likelihoods (fails)

Find the sum of the log-likelihoods 

$\sum{(IL(y;μ,σ))} = {log}(\prod{L(y;μ,σ)})$

Get model with maximum log-likelihood

Deviance is a measure of goodness-of-fit (equivalent to $\sum{\epsilon ^2}$)

## Generalised Linear Model (GLM)

 - Fitted with Maximum likelihood
 - Suitable for various distributions
 - Variance function
 - Link function transforms response
 - Linear predictor

 
$y_i = β_0~ + β_1x_i + ε_i$

$g(E(y_i)) = β_0 + β_1x_i$

## Distributions & links  (E(y) = μ){.columns-2}

__Normal (gaussian)__

 - y continuous
 - All values
 - $\mu = E(y_i) = β_0 + β_1x_i$
 - $var(y) = σ^2$

__Poisson__

 - Discrete
 - Counts
 - $log(\mu) = β_0 + β_1x_i$
 - $var(y) = \mu$

__Binomial__

 - Success/failure
 - $log(\frac{\mu}{1 - \mu}) = β_0 + β_1x_i$
 - $var(y) = n\mu(1 - μ)$
 
## Binomial distribution{.columns-2}

Success = 1 & Failure = 0

n = trials 

- number of Successes + Failures in an experiment. 
- Special case n = 1 -- Bernoulli trial

$p$ = probability of success
 
$(1 - p)$ = probability of failure

$μ = np$ 

$var(y) = np(1 - p)$ 
 
__Suggest some types of data that you would expect to follow a binomial distribution__

```{r, echo = FALSE, fig.height=4, fig.width=5}
surv <- read.csv("practicals/data/survival.csv")
ggplot(surv, aes(temp, surv)) + 
  geom_point() + 
  labs(x = "Temperature", y = "Survival")
```

##
Use logistic link:

 - $log(\frac{p}{1 - p}) = \beta_0~ + \beta_1x_i$
 
 - $p = \frac{exp(β_0 + β_1x_i)}  {1 + exp(β_0 + β_1x_i)}$

Response = Survival (i.e. success)

Predictor = temperature
```{r}
fit_glm <- glm(surv ~ temp, data = surv, family = binomial)
```

y is 

 - vector of 0 & 1
 - factor representing success and failure
 - vector giving the proportion of successes if multiple trials are used. Weights should give number of trials
 - two column matrix giving number of successes and failures
 
##
 
```{r}
 anova(fit_glm)
```
 
##
 
```{r}
summary(fit_glm)
```

##
Response:

 $p = \frac{exp(β_0 + β_1x_i)} { 1 + exp(β_0 + β_1x_i)}$
 
```{r, fig.height=4, fig.width=5} 
ggplot(cbind(surv, fit = fitted(fit_glm)), aes(x = temp, y = surv)) +
  geom_point() +
  geom_line(aes(y = fit), colour = "red")
```

## Separation

Predictor perfectly separates zeroes and ones in response

```{r}
#| warning: true 
df <- data.frame(x = 1:20, y = rep(c(0, 1), each = 10))
mod <- glm(y ~ x, data = df, family = binomial)
```
```{r, echo = FALSE, fig.height=3, fig.width=5}
ggplot(cbind(df, fit = fitted(mod)), aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = fit), colour = "red")

```

## Separation

Very high coefficient

Very large uncertainty

 - too many categorical predictors?
 - maximum penalized likelihood package `brglm`
 - Bayesian analysis `bayesglm` in `arm` package, rstan

## Poisson distribution{.columns-2}

y = discrete (counts)

x = predictor 

$mean = \mu$

$var(y) = \mu$

$log(\mu) = β_0 + β_1x_i$

```{r, echo = FALSE, fig.height=4, fig.width=4}
x <- 0:10
y <- c(dpois(x, lambda = 1.5), dpois(x, lambda = 4))
lambda <- rep(c(1.5, 4), each = length(x))
df <- data.frame(x = factor(x), y, lambda = factor(lambda))
ggplot(df, aes(x, y, fill = lambda)) + 
  geom_col(position = "dodge") + 
  scale_x_discrete(breaks = 0:10, labels=0:10) +
  labs(y = "Probability", fill = "Lambda", x = "Response")
```

Shape depends on lambda

With large values of lambda, normal approximation is possible

__Suggest some data types that are Poisson distributed__

## Poisson example - Number of species vs. cover

```{r}
rich <- read.csv("practicals/data/sorich.csv")
str(rich)

```

Predictor is cover

Response is number of species

##
```{r}
ggplot(rich, aes(x = cover, y = nsp)) + 
  geom_point() + 
  labs(y = "Species Richness", x = "Cover")
```

##
```{r}
fit_glm <- glm(nsp ~ cover, data = rich, family = poisson)
anova(fit_glm, test = "Chisq")
```

##
```{r}
summary(fit_glm)
```

##
```{r, fig.height=4, fig.width=5}
ggplot(cbind(rich, fit = fitted(fit_glm)), aes(x = cover, y = nsp)) + 
  geom_point() + 
  geom_line(aes(y = fit), colour = "red") +
  labs(y = "Species Richness", x = "Cover")
```

log(μ) = β~0~ + β~1~x~i~

## Overdispersion
Variance in Binomial and Poisson models is specified: 

 - Poisson $var(y) = μ$
 - Binomial $var(y) = nμ(1 - μ)$

Real data can have more (or less) variance than this if not drawn from a pure Binomial or Poisson process.

 - Overdispersed data has more variance than expected
 - Overdispersion makes statistical tests too liberal. Increased risk of type I error.
 
## Detecting Overdispersion

Overdispersion if Residual deviance > residual df
```{r}
anova(fit_glm)

performance::check_overdispersion(fit_glm)
```

## Overdispersion

__What is it:__

 - A dependency between observations
 - Missing parameter(s)

__When:__

 - Poisson
 - Binomial if n >1
 
---

__What to do:__

Find missing parameters

Quasi-likelihood:

 - quasipoisson
 - quasibinomial


Negative binomial - counts\
Beta-binomial - proportion data\
Individual level random effects


Zero-inflated distributions

Hurdle models

## Quasi models

Poisson: $var(y) = μ$

Quasipoisson: $var(y) = \phi\mu$

Scale parameter $\phi$ modifies relationship between mean and variance

```{r}
glm_fit1 <- glm(nsp ~ cover, data = rich, family = quasipoisson) 
```

Fitted model will be the same

Confidence intervals will be wider

P values will be higher

No AIC for model selection. Quasi-AIC?

## Negative binomial

Discrete probability distribution - number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified  number of failures occurs.

Two parameters:

 - p of Bernoulli trials
 - r failures

Implemented in package `MASS`
```{r, message = FALSE}
library("MASS")
fit_nb <- glm.nb(nsp ~ cover, data = rich, link = log)
broom::tidy(fit_nb)
```

Has AIC

## Individual level random effect

Fit a mixed effect model when the random effect is each individual

```{r, message = FALSE}
library("lme4")
rich <- rich |> mutate(id = 1:n())
fit_mx <- glmer(nsp ~ cover + (1 | id), data = rich, family = poisson)
broom.mixed::tidy(fit_mx)
```

[Harrison (2015)](https://doi.org/10.7717%2Fpeerj.1114)

## Zero-inflated models

Used to model count data that has an excess of zero counts. 

 - Excess zeros generated by a separate process 
 - Excess zeros modelled independently.  

Two component models

 - Poisson/Negative binomial component
 - Binomial component gives excess zeros
 
Fit with package `pscl` function `zeroinfl()`

``` r
# formula = response ~ count_predictors | zero_predictors
zeroinfl(art ~ . | 1, data = bioChemists)

```


## Offset

Counts are rates with uneven amount of time (or area)

Double the time $\approx$ double the count - coefficient known to be 1

$$log(\frac{\mu_i}{t_i}) = \beta_0 + \beta_1x$$
$$log(\mu_i) - log(t_i) = \beta_0 + \beta_1x$$
$$log(\mu_i) = \beta_0 + \beta_1x) + \beta_2 \times  log(t_i) $$
``` r
glm(y ~ x + offset(log(time)), family=poisson)
```

##

```{r}
library(MASS) # for Insurance dataset

ggplot(Insurance, aes(x = Holders, y = Claims, colour = Age)) + geom_point()
```

##
```{r}
# glm with offset
glm(Claims ~ District + Group + Age + offset(log(Holders)),
    family=poisson, data=Insurance)


```

## Predictions

By default, predictions are on linear-predictor scale
```{r}
fit_glm <- glm(nsp ~ cover, data = rich, family = poisson)
(pred <- predict(fit_glm, newdata = data.frame(cover = 10)))
```

Transform with inverse of link to put on to response scale

Poisson uses log link

$log(\mu) = \beta_0 + \beta_1x_i$

Use exponential function
```{r}
exp(pred)
```

Or ask for predictions on reponse scale
```{r}
(pred <- predict(fit_glm, newdata = data.frame(cover = 10), type = "response"))
```

##

Link scale (linear predictor scale)

```{r,echo = FALSE}
newD <- seq(min(rich$cover), max(rich$cover), length = 100)
pred <- predict(fit_glm, newdata = data.frame(cover = newD), se.fit = TRUE)
#upper 95%ci
up <- pred$fit + 1.96 * pred$se.fit
#lower 95%ci
down <- pred$fit - 1.96 * pred$se.fit
```

```{r, fig.height=4, fig.width = 5}
ggplot(rich, aes(cover, log(nsp))) +
  geom_point() +
  geom_line(aes(x, y = fit), data  = data.frame(x = newD, fit = pred$fit), inherit.aes = FALSE)  
```

--- 

Response scale (linear predictor scale)

```{r, fig.height=4, fig.width = 5}
ggplot(rich, aes(x = cover, y = nsp)) +
  geom_point() +
  geom_line(aes(x, y = fit), data  = data.frame(x = newD, fit = exp(pred$fit)), inherit.aes = FALSE)
```


## Confidence interval

Estimate $\mu$ and SE on link scale {$log(\mu)$ & SE}

Estimate confidence interval ($\mu ± 2*SE$)

Use link-function to obtain values on 'response scale'  {$exp(\mu ± 2*SE)$}
```{r}
(pred <- predict(fit_glm, newdata = data.frame(cover = 10), se.fit = TRUE))
```

##
```{r}
#estimate
exp(pred$fit)

#upper 95%ci
exp(pred$fit + 1.96 * pred$se.fit)

#lower 95%ci
exp(pred$fit - 1.96 * pred$se.fit)
```

##

Link scale (linear predictor scale)

```{r,echo = FALSE}
newD <- seq(min(rich$cover), max(rich$cover), length = 100)
pred <- predict(fit_glm, newdata = data.frame(cover = newD), se.fit = TRUE)
#upper 95%ci
up <- pred$fit + 1.96 * pred$se.fit
#lower 95%ci
down <- pred$fit - 1.96 * pred$se.fit
```

```{r, fig.height=4, fig.width = 5}
ggplot(rich, aes(cover, log(nsp))) +
  geom_point() +
  geom_line(aes(x, y = fit), data  = data.frame(x = newD, fit = pred$fit), inherit.aes = FALSE) + 
  geom_ribbon(aes(x, ymax = up, ymin = down), data  = data.frame(x = newD, up, down), inherit.aes = FALSE, alpha = 0.2)  
```


--- 

Response scale (linear predictor scale)

```{r, fig.height=4, fig.width = 5}
ggplot(rich, aes(cover, nsp)) +
  geom_point() +
  geom_line(aes(x, y = fit), data  = data.frame(x = newD, fit = exp(pred$fit)), inherit.aes = FALSE) + 
  geom_ribbon(aes(x, ymax = up, ymin = down), data  = data.frame(x = newD, up = exp(up), down = exp(down)), inherit.aes = FALSE, alpha = 0.2)  
```

## Confidence intervals for Binomial

Analogous to calculation for Poisson

Use Logistic link function

$log(\frac{p}{1-p})= β_0 + β_1x_i$

$p = \frac{exp(β_0 + β_1x_i)} {1 + exp(β_0 + β_1x_i)}$

```{r, eval = FALSE}
# using augment & plogis
broom::augment(fit_glm, se_fit = TRUE, type.predict = "link") |> 
  mutate(upper = plogis(.fitted + 1.96 * .se.fit),
         lower = plogis(.fitted - 1.96 * .se.fit), 
         fitted = plogis(fitted))

```



## Distributions

:::: {.columns}

::: {.column width="50%"}
Normal

 - Continuous
 - Constant variance
 - All numbers

Poisson

 - Discrete (counts)
 - Variance = $mu$
 - Positive numbers

:::

::: {.column width="50%"}
Binomial

 - Success / failure trials
 - one-to-many trials per observation
 - variance = $\mu(1 - \mu)$

Gamma

 - Continuous
 - positive numbers
 - variance $\propto \mu ^ 2$
 
Log normal 

 - `family = gaussian(link = "log")`
:::

::::



 
## Generalised additive models

Assume smooth response to predictor

Data-driven

Don't know what reponse should be

Exploration of data

Can take any distribution and link function available for GLM

---

Smooth terms

 - s()
 - y ~ s(x)

Complexity of smooth terms estimated using Generalised cross-validation

Omit the smooth term and get a GLM!

lot more on GAMs at [https://github.com/gavinsimpson/adelaide-2017/blob/master/02-tuesday/02-gams.pdf](https://github.com/gavinsimpson/adelaide-2017/blob/master/02-tuesday/02-gams.pdf)


Try the course on Generalised Additive Models in R at [https://noamross.github.io/gams-in-r-course/](https://noamross.github.io/gams-in-r-course/)


## GAM in R

```{r}
pot <- read.csv("practicals/data/pot.csv")
library(mgcv)
mod <- gam(potalp ~ s(alt), data = pot, family = binomial)
mod2 <- gam(potalp ~ alt + I(alt^2), data = pot, family = binomial)
anova(mod, mod2, test = "Chisq")
```

---

```{r}
ggplot(cbind(pot, fit.gam = fitted(mod), fit.glm = fitted(mod2)), aes(alt, potalp)) +
  geom_point() +
  geom_line(aes(y = fit.gam, colour = "GAM")) +
  geom_line(aes(y = fit.glm, colour = "GLM")) +
  labs(x = "Elevation", y = "P. alpina", colour = "Model")
```
